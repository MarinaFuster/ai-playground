{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß± Building a GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìò This notebook contains the code for implementing a transformer neural network, trained on a small dataset, composed by Shakespeare literature. It is based on the awesome tutorial from Andrej Karpathy, available in his [Youtube Channel](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=227s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset available at https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 200 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(list(set(text))) # get all unique characters\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(''.join(vocabulary))\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê Tokenization\n",
    "The tokenization process involves having an \"encoder\" and \"decoder\" that will transform an input (chunk of text) into a list of integers.\n",
    "\n",
    "In this example, the vocabulary is built at word level, from the dataset. It is the total amount of distinct characters. The encoding is pretty simple: each character returns its index in a sorted list that represents the vocabulary.\n",
    "\n",
    "- \"Hello\" will return a list with 5 elements.\n",
    "- \"Hello there\" will return a list with 11 elements.\n",
    "\n",
    "The elements inside the list can be integers between 0 and 64, because we only have 65 elements in the vocabulary.\n",
    "\n",
    "There are other tokenization strategies and the most widely used are chunk-level (grup of characters) instead of letter or word level.\n",
    "\n",
    "Google's tokenizer is SentencePiece, available [here](https://github.com/google/sentencepiece), and OpenAI's tokenizer is TikToken, available [here](https://github.com/openai/tiktoken)\n",
    "\n",
    "There's a trade-off between the vocabulary size and the sequence length after encoding. In TikToken, you have 50257 tokens, which means that \"Hello\" will return a shorter encoder list when compared to our custom encoding process, which is at character level.\n",
    "\n",
    "##### ‚ùì Open Questions\n",
    "- How do you end up having inputs of the same length if the encoding returns lists of different size?\n",
    "- Is the trade-off always applicable? Couldn't that depend on the input text you are trying to encode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(vocabulary) }\n",
    "itos = { i:ch for i,ch in enumerate(vocabulary) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for OpenAI's GPT-2: 50257\n",
      "Encoding with OpenAI's tiktoken returns shorter list\n",
      "[71, 4178, 612]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "\n",
    "openai_encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"Vocabulary size for OpenAI's GPT-2: {openai_encoder.n_vocab}\")\n",
    "print(\"Encoding with OpenAI's tiktoken returns shorter list\")\n",
    "print(openai_encoder.encode(\"hii there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset is now completely encoded. Shape: torch.Size([1115394]), Type: torch.int64\n",
      "Let's look at the first 150 characters: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "\n",
    "encoded_dataset = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'Our dataset is now completely encoded. Shape: {encoded_dataset.shape}, Type: {encoded_dataset.dtype}')\n",
    "print(f\"Let's look at the first 150 characters: {encoded_dataset[:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Train Test Split\n",
    "We are now performing the usual train test split and something interesting about it is that train data is getting 90% of the dataset without shuffling. How do we deal with \"shuffle\" if text is supposed to be sorted in order for it to make sense? The data will be divided into \"chunks\" of a specific length and what the network will receive are randomly chosen \"chunks\" that represent the context for the model to predict what comes next. The chunks or context have a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(encoded_dataset)) # first 90% will be train, rest val\n",
    "train_data = encoded_dataset[:n]\n",
    "val_data = encoded_dataset[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 8\n",
    "train_data[:context_length+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ‚ùì Why do we have (n+1) tokens if context length is n? The idea is to have 8 examples of \"prediction\". When \"18\" is presented, we want the model to predict \"47\", when \"18, 47\" is presented, we want the model to predict \"56\", and so on. With 9 characters, there are 8 prediction instances. Another question that can come up is why do we train the transformer with increasing length examples? The benefit of doing it this way is that the transformer can predict with different context length. If the model gets one token or multiple tokens, it can handle the situation at inference time because it was trained for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length+1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1] # all elements up to t (including it)\n",
    "    target = y[t] # expected value at time t\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "The inputs and targets create 32 independent examples that will be processed in parallel.\n",
      "These train examples being fed into the network are of variable length, as discussed before.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "print(f\"The inputs and targets create {batch_size*block_size} independent examples that will be processed in parallel.\")\n",
    "print(\"These train examples being fed into the network are of variable length, as discussed before.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Language Model\n",
    "One of the simplest models to implement are N-gram models. The N stands for how many tokens are \"looked at\" in the sequence to predict the next token. That number is N-1 so, for example, in a Bigram Language Model, it looks to the previous token in order to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "We are expectinga loss of -ln(1/65) which is approximately 4.17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # we need to format the logits and targets properly for the loss function which excpects the Channels to be the second dimension\n",
    "            logits = logits.view(B*T, C) # 32 x 65 dimension\n",
    "            targets = targets.view(B*T) # 32 dimension\n",
    "            loss = F.cross_entropy(logits, targets) # this is negative log likelihood loss\n",
    "\n",
    "        # logits are scores, NOT probabilities\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocabulary_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(\"We are expectinga loss of -ln(1/65) which is approximately 4.17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùì Logits will return a tensor of (B,T,C) dimensions. B corresponds to 4, T to 8, and C to 65. What do these dimensions represent?\n",
    "- B: stands for batch size\n",
    "- T: stands for time and is the same number as context length\n",
    "- C: stands for channel and represents the vocabulary size\n",
    "\n",
    "Logits will contain the scores for the next character in the sequence, in order to answer the question: Which character is more likely to be next?\n",
    "Something important to notice in a bigram model is that the process is completely parallel because each token can infer what comes next by the fact that they know they are token X.\n",
    "\n",
    "If I'm token 5, I can infer what comes next because it will be defined in the row corresponding to that index, when looking at the token_embedding_table.\n",
    "If the input was only [5], then logits will be of shape 1x65 because it returns the fifth row in token_embedding_table. \n",
    "But, in our case, the entire input is of size BxT or 4x8 (32), as discussed before. This model is very simple which means that getting the actual probability for each item in the vocabulary is just looking up for that row in the table because there is no extra context I actually care about. This will change for models with greater context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's now generate some text, given a starting context. Note it is a stochastic model.\n",
      "Encoded context: tensor([24, 24, 24, 24]) and decoded context is: LLLL.\n",
      "Time, of value 4, controls the length of the context. Total sequence will have 14 tokens.\n",
      "LLLLbL?qP-QWkt\n",
      "LLLLID&viYDEsx\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's now generate some text, given a starting context. Note it is a stochastic model.\")\n",
    "batch = 2\n",
    "time = 4\n",
    "encoded_context = torch.full((batch, time), 24) # 24 is a valid index (between 0 and 65)\n",
    "print(f\"Encoded context: {encoded_context[0]} and decoded context is: {decode(encoded_context[0].tolist())}.\")\n",
    "\n",
    "max_new_tokens = 10\n",
    "print(f\"Time, of value {time}, controls the length of the context. Total sequence will have {time+max_new_tokens} tokens.\")\n",
    "generated_sequences = m.generate(encoded_context, max_new_tokens=max_new_tokens) # one for each element in batch\n",
    "for sequence_index in range(batch):\n",
    "    print(decode(generated_sequences[sequence_index].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùì Why does generate method get the entire context if this is a Bigram model?\n",
    "It is a fair question to ask, given that prediction is based on the last element as the comment indicates in the function: we are focusing ONLY on the last time step. The idea is that this generate function intends to be general to be re-used later. When we create a model that uses a longer context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ö° Training Process\n",
    "Here detail the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # this should be smaller for larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.471229314804077\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(15000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    input_batch, target_batch = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(input_batch, target_batch)\n",
    "    optimizer.zero_grad(set_to_none=True) # each step resets the gradients to compute them again\n",
    "    loss.backward() # backpropagate the loss\n",
    "    optimizer.step() # based on the gradients, take a step in the right direction\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded context: tensor([24]) and decoded context is: L.\n",
      "LI eseear t by tit thodselegrt R:\n",
      "Fo\n",
      "\n",
      "\n",
      "r bunour ther w, wequghobatho s llops t t toras bd ily,\n",
      "TEY ishwarod, se ttha's; I ppry memitth we ieeelo me,\n",
      "Fait Cloog se tt afre ce tim wary stuklel d lofran VID g\n",
      "OLI'tof areris nde imowlmandise wineatingiomanh y Mave,\n",
      "NCHegr.\n",
      "Trs y thaurymeresththonglast ff\n",
      "LLorlout an\n",
      "INSun IDUpod:\n",
      "An ug t uly hme hondanok. orooniowor WAn; y tangghe-d e hixarr howinncotisharomatre he sw I ayoty he, o lyofug, we VINUCORIO,\n",
      "Yofr Seret on melowe;\n",
      "IO:\n",
      "he'sghork.\n",
      "SLow arehauginduld I w afoisor d\n",
      "JAnfucuesond sayoullies ldiaritond sofay\n",
      "Abothanses bledilin:\n",
      "Whas?\n",
      "BELUEELORLI\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "time = 1\n",
    "encoded_context = torch.full((batch, time), 24) # 24 is a valid index (between 0 and 65)\n",
    "print(f\"Encoded context: {encoded_context[0]} and decoded context is: {decode(encoded_context[0].tolist())}.\")\n",
    "\n",
    "max_new_tokens = 300\n",
    "generated_sequences = m.generate(encoded_context, max_new_tokens=max_new_tokens) # one for each element in batch\n",
    "for sequence_index in range(batch):\n",
    "    print(decode(generated_sequences[sequence_index].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé© Mathematical Trick for Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to get a way of the tokens \"talking\" to each other. This \"talking\" is limited to previous context. We do not have any information from the future because that is what we are trying to predict.\n",
    "\n",
    "Let's imagine I am a token. One way to understand my past would be to average my channel with the cannels from the tokens in my history.\n",
    "If the context is [14, 56, 19, 24, 32] and I am token \"24\", then I would average channel[14], channel[56], channel[19] and channel[24]. There are many downsides to this kind of interaction (for example, loss of spatial interaction) but let's assume we are using that for now.\n",
    "\n",
    "Side not: bow is bag of words, an expression people use to indicate averaging a bunch of things together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "fake_logits = torch.randn(B,T,C)\n",
    "fake_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "fake_logits_averaged_with_previous_context = torch.zeros((B,T,C))\n",
    "for batch_element in range(B): # each sample from the batch\n",
    "    for time_step in range(T): # each time step\n",
    "        fake_previous_logits = fake_logits[batch_element,:time_step+1] # (t,C)\n",
    "        fake_logits_averaged_with_previous_context[batch_element, time_step] = torch.mean(fake_previous_logits, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample from the batch:\n",
      "Time 0, logit: tensor([ 0.1808, -0.0700]), averaged: tensor([ 0.1808, -0.0700])\n",
      "Time 1, logit: tensor([-0.3596, -0.9152]), averaged: tensor([-0.0894, -0.4926])\n",
      "Time 2, logit: tensor([0.6258, 0.0255]), averaged: tensor([ 0.1490, -0.3199])\n",
      "Time 3, logit: tensor([0.9545, 0.0643]), averaged: tensor([ 0.3504, -0.2238])\n",
      "Time 4, logit: tensor([0.3612, 1.1679]), averaged: tensor([0.3525, 0.0545])\n",
      "Time 5, logit: tensor([-1.3499, -0.5102]), averaged: tensor([ 0.0688, -0.0396])\n",
      "Time 6, logit: tensor([ 0.2360, -0.2398]), averaged: tensor([ 0.0927, -0.0682])\n",
      "Time 7, logit: tensor([-0.9211,  1.5433]), averaged: tensor([-0.0341,  0.1332])\n"
     ]
    }
   ],
   "source": [
    "print(f\"A sample from the batch:\")\n",
    "for time_step in range(T):\n",
    "    print(f\"Time {time_step}, logit: {fake_logits[0][time_step]}\" \\\n",
    "        f\", averaged: {fake_logits_averaged_with_previous_context[0][time_step]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to make this averaging more efficient. We can transform the operation into matrix multiplication by using triangular matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logits:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "Weighted matrix:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "Averaged logits:\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "These are for a batch of 1 element, context length 3 and vocabulary of 2 elements.\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # this will give a lower triangular matrix\n",
    "# we want to normalize each row because the result will be that each coefficient in the\n",
    "# i-th row tells us how much each logit in the upper and current row contributes.\n",
    "a = a / torch.sum(a, dim=1, keepdim=True) # dim means sum along the columns\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(f\"Original logits:\\n{b}\")\n",
    "print(f\"Weighted matrix:\\n{a}\")\n",
    "print(f\"Averaged logits:\\n{c}\")\n",
    "print(\"These are for a batch of 1 element, context length 3 and vocabulary of 2 elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "lower_triangular = torch.tril(torch.ones(T, T))\n",
    "weights = lower_triangular / torch.sum(lower_triangular, 1, keepdim=True)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logits shape is {logits.shape}\n",
      "Averaged logits shape is torch.Size([32, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "inputs, _ = get_batch('train') # 4 elements\n",
    "logits, _ = m.forward(inputs, targets=None)\n",
    "print(\"Original logits shape is {logits.shape}\") # B, T, C\n",
    "\n",
    "logits_averaged_with_previous_context = weights @ logits\n",
    "# weights is (T, T) and logits is (B, T, C). What this matrix multiplication does is\n",
    "# include a batch dimension in weights, making it (B, T, T) and then, for each batch element\n",
    "# it computes the weighted average by multiplying the original weights (T, T) with the current (T, C)\n",
    "# the result would be (B, T, C)\n",
    "print(f\"Averaged logits shape is {logits_averaged_with_previous_context.shape}\") # B, T, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's another way of computing the same operation, which is using the softmax operation. In this scenario, we get a sort of \"lower triangular\" matrix but, where the original one has zeros, this one has -inf, and where the original one has ones, this one has zeros. What softmax does is e^coefficient (e^0 is 1, e^-inf is 0) and then divide by the sum of elements in the row. Basically, this returns the exact same result.\n",
    "\n",
    "‚ùìThe question is... why would you want to do it this way? What's different or more efficient compared to the previous one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# another version is using softmax\n",
    "weights_v2 = torch.zeros((T,T))\n",
    "weights_v2 = weights_v2.masked_fill(lower_triangular == 0, float('-inf')) # the masked indicates we can't look into the future\n",
    "weights_v2 = F.softmax(weights_v2, dim=-1)\n",
    "logits_averaged_with_previous_context_v2 = weights_v2 @ logits\n",
    "# this will return True if both elements are equal\n",
    "print(torch.allclose(logits_averaged_with_previous_context, logits_averaged_with_previous_context_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our current version, there's no distinction in how interesting another token is for the token in question. This is what's called \"affinity\". So far, the \"attention\" is equal to all tokens in the previous context (including this one) and there's zero \"attention\" to the tokens in the future. The future can't communicate with the past. The conclusion here is that you can make weighted aggregations from the past by using matrices in the lower triangular format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
